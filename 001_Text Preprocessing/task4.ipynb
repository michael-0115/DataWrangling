{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and preprocess abstracts\n",
    "\n",
    "Abstract provides important information that can assist in learning the citation network. So, our main task for task 4 is to extract all the abstracts for all the patents in the XML file which contain contains 2,500 patents in total.\n",
    "And store those abstracts as sparse count vectors to produce a output file called “count_vectors.txt”, content inside starting with patent_id for each patent, and patent_id followed by “word_index:count” pairs. Also output another file named “vocab.txt” for all the vocabularies with the index that been shown in the “count_vectors.txt” as word_index for cross reference. The “vocab.txt” output format are each vocabulary starting with word_index followed by word and line by line for each vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Import Libraries we need for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer  \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from itertools import chain\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import *\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Use BeautifulSoup to extract information from the XML file.\n",
    "\n",
    "##### 2.1 Here we are using the same parse method as task 1 we use to parse and extract data from XML files with Python. Use BeautifulSoup to extract information from the XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"./patents.xml\"),\"html.parser\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the hierarchy of the xml file, we noted that the information we would like to extract are stored in the following tags:\n",
    "\n",
    " * for patent's ID:  \n",
    " \n",
    "        publication-reference > doc-number \n",
    "        (the 2,500 patents' id is located under tags publication-reference > doc-number)\n",
    " \n",
    " * for patent's abstracts: \n",
    "              \n",
    "        abstract > p \n",
    "                 > p\n",
    "                  .\n",
    "                  .\n",
    "                 > p\n",
    "        (a patent may contains more than one abstract data in p tags under patent's abstract tag)\n",
    "        \n",
    "##### 2.2 To get the patent_id and return to a list: patents_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PP021722', u'RE042159', u'RE042170', u'07891018', u'07891019']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication_reference_tags = soup.find_all(\"publication-reference\")\n",
    "patents_id = [item.find(\"doc-number\").string for item in publication_reference_tags] \n",
    "patents_id[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3  To get patent's abstracts and return to a list as a item for each patent: ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'A new apple tree named Daligris is disclosed. The fruit of the new variety is particularly notable for its eating quality and distinctive flavor and appearance. The fruit is very sweet and has a pronounced aniseed flavor, and takes on a distinctive red orange coloration as it ripens on the tree.',\n",
       " u'A sensing device includes a circuit that compensates for time and spatial changes in temperature. The circuit includes elements to correct for variation in permeability of a highly permeable core of a differential variable reluctance transducer as temperature changes. The circuit also provides correction for temperature gradients across coils of the transducer.',\n",
       " u'At least one peripheral processing apparatus and at least one information processing apparatus, interconnected through a network, include a storage means for storing control information by which the information processing apparatus controls the peripheral apparatus through the network. The control information stored in the storage means is transferred through the network to the information processing apparatus, which receives it, the control data being generated by the information processing apparatus based upon the control information transferred to the information processing apparatus control means executes control process according to the data control received.',\n",
       " u\"A knee protective device for garments comprising of at least one pocket in the vicinity of the knees having an opening for receiving an insert for protecting the wearer's knees.\",\n",
       " u'A heatable garment having a plurality of layers is shown. The first layer is an outer fabric layer of any fabric which is lightweight, durable and treatable and may also be air impervious. Cooperating with the outer fabric layer is an envelope or inflatable bladder having a plurality of interconnected chambers. A further inner layer adjacent the envelope or bladder is a heat producing element such as a plurality of imbedded heating wires woven into a supporting fabric. The supporting fabric may, however, comprise the inner wall of the inflatable envelope. A source of heat energy, such as a battery, is connected to the heat producing element to heat the inner fabric liner. Inflation of the inflatable bladder urges the heat producing element into intimate contact with the body of the wearer. The inflatable bladder also form fits the garment on the body of the wearer and provides insulation against heat loss.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_tags = soup.find_all(\"abstract\")\n",
    "p_tags = [item.find_all(\"p\") for item in ab_tags] \n",
    "\n",
    "ab = []\n",
    "for l in p_tags:\n",
    "    if len(l) == 1:\n",
    "        ab.append((l[0].get_text().encode('ascii', 'ignore').decode('ascii')))   \n",
    "                                                    # convert Unicode to ASCII \n",
    "    else:\n",
    "        temp = [tag.get_text().encode('ascii', 'ignore').decode('ascii') for tag in l] # convert Unicode to ASCII \n",
    "        ab.append(\" \".join(temp))\n",
    "\n",
    "ab[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Text pre-processing for patent's abstract.\n",
    "\n",
    "Since now we have extracted all the abstract data for the 2,500 patents and the data are in our hand. \n",
    "We will then come to the main stage of Text Pre-Processing. This text pre-processing task if for us to generate a structured and suitable format for the data given that to provide some useful information in the output file for next stage of analysis as our final goal.\n",
    "\n",
    "### We are using below steps for the text pre-processing process for patents' abstracts: \n",
    "\n",
    "#### 3.1 Tokenization (sentences > words) and Case normalization\n",
    "    * sentence tokenization\n",
    "    * words tokenization (the words tokenization uses regular expression paired with RegexpTokenizer to get the sentences to be tokenized to tokens. \n",
    "    * lower all tokens to lower case.\n",
    "#### 3.2 Stemming and Lemmatization\n",
    "    * use different stemmers and lemmar tool to reduce some different forms of a word to a common base form to reduce the complexity of the word's document. After compared the different results from each stemmers and lemmar. I decided to WordNetLemmatizer as it return a better, more suitable result of the words Stemming and Lemmatization while the three stemmers return some wired word transformation result in not been adopted in this text pre-processing process. \n",
    "#### 3.3 Producing meaningful bigram collocation\n",
    "    * before removing stop words, first is to use the BigramAssocMeasures to get all combination of bigrams. After adjusting some setting in the BigramAssocMeasures to produce a better set of bigrams, choosing 300 bigrams to the list. Do some more filtering to remove bigram which are not apparently not meaningful (bigram in stopwords) and not logical (bigram combined with two same words) to get a final list of suitable bigram, 122 meaningful bigrams in total.\n",
    "#### 3.4 Removing Stop words\n",
    "    * once obtained the bigrams, we can remove the Stop words which are words that are extremely common and carry little lexical content. They are usually referred to as function words in linguistics that not useful for telling us about the meaning of the text. \n",
    "#### 3.5 Tokenization (tokens updated to bigrams)\n",
    "    * use MWETokenizer to produce bigrams to the abstract token list by replacing its unigram form \n",
    "#### 3.6 Removing the Most and Less Frequent Words\n",
    "    * the top-20 most frequent words based on word’s document frequency, and words only appearing in one abstract will be removed from the abstracts token list as task requirement before generating the files as the text-preprocessing output.\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start:\n",
    "#### 3.1 Tokenization (sentences > words ) and Case normalization:\n",
    "###### 3.1-a  sentence tokenization for each abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'A new apple tree named Daligris is disclosed.',\n",
       "  u'The fruit of the new variety is particularly notable for its eating quality and distinctive flavor and appearance.',\n",
       "  u'The fruit is very sweet and has a pronounced aniseed flavor, and takes on a distinctive red orange coloration as it ripens on the tree.'],\n",
       " [u'A sensing device includes a circuit that compensates for time and spatial changes in temperature.',\n",
       "  u'The circuit includes elements to correct for variation in permeability of a highly permeable core of a differential variable reluctance transducer as temperature changes.',\n",
       "  u'The circuit also provides correction for temperature gradients across coils of the transducer.'],\n",
       " [u'At least one peripheral processing apparatus and at least one information processing apparatus, interconnected through a network, include a storage means for storing control information by which the information processing apparatus controls the peripheral apparatus through the network.',\n",
       "  u'The control information stored in the storage means is transferred through the network to the information processing apparatus, which receives it, the control data being generated by the information processing apparatus based upon the control information transferred to the information processing apparatus control means executes control process according to the data control received.'],\n",
       " [u\"A knee protective device for garments comprising of at least one pocket in the vicinity of the knees having an opening for receiving an insert for protecting the wearer's knees.\"],\n",
       " [u'A heatable garment having a plurality of layers is shown.',\n",
       "  u'The first layer is an outer fabric layer of any fabric which is lightweight, durable and treatable and may also be air impervious.',\n",
       "  u'Cooperating with the outer fabric layer is an envelope or inflatable bladder having a plurality of interconnected chambers.',\n",
       "  u'A further inner layer adjacent the envelope or bladder is a heat producing element such as a plurality of imbedded heating wires woven into a supporting fabric.',\n",
       "  u'The supporting fabric may, however, comprise the inner wall of the inflatable envelope.',\n",
       "  u'A source of heat energy, such as a battery, is connected to the heat producing element to heat the inner fabric liner.',\n",
       "  u'Inflation of the inflatable bladder urges the heat producing element into intimate contact with the body of the wearer.',\n",
       "  u'The inflatable bladder also form fits the garment on the body of the wearer and provides insulation against heat loss.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "ab_sent = [sent_tokenize(string) for string in ab]  \n",
    "ab_sent[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.1-b  word tokenization for each sentence and case normalization for each token when tokenized\n",
    "\n",
    "Before tokenization, we first to look the contents of the abstract, and decided how to do the tokenization so we can get the first raw tokens for us to proceed the next text pre-processing.\n",
    "\n",
    "As the task is to extract the patents abstract for the purpose of getting some information about the patent's network, which means to getting to know the relationship between patents. For this reason, words are important while the figures such as number in digit are not important for extract the information of relationship of patents.\n",
    "So, the token strategy is to token out the word and give up all the figures in the raw text. \n",
    "In addition, \n",
    "\n",
    "a) for words have hyphens in between will be treated as a whole word: \n",
    "* high-torque\n",
    "* case-packer\n",
    "* case-blank\n",
    "* Metal-Oxide-Silicon\n",
    "\n",
    "b) for words have slash in between will be treated as multiple words: \n",
    "* insert/standard\n",
    "* picture/booklet\n",
    "\n",
    "c) for words have apostrophe will be tokened to obtain the \"Subject\" words without character after the apostrophe,\n",
    "   as the Contractions and Possessives word that with apostrophe provide no different meaning for as in the patent citation network task, even the obtained \"Subject\" word will be removed is the it is a stopword:\n",
    "* Contractions: she'll -> she\n",
    "* Possessives: cat's -> cat\n",
    "###### The above a) to c) will use the regular expression to dealing with the tokenization:  (?:[A-Za-z]+[\\-]?[A-Za-z]+[\\-]?)+ \n",
    "\n",
    "d) for the abbreviations will be treated as a whole word: \n",
    "* U.S.A\n",
    "* B.B.C\n",
    "* i.e.\n",
    "* e.g.\n",
    "###### The above d) will use the regular expression to dealing with the tokenization: regular expression: (?:[A-Za-z]+[\\.]?){2,}\n",
    "###### And for the rest non digit word will use the regular expression to dealing with the tokenization: regular expression: (?:[A-Za-z])+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'a',\n",
       "  u'new',\n",
       "  u'apple',\n",
       "  u'tree',\n",
       "  u'named',\n",
       "  u'daligris',\n",
       "  u'is',\n",
       "  u'disclosed',\n",
       "  u'the',\n",
       "  u'fruit',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'new',\n",
       "  u'variety',\n",
       "  u'is',\n",
       "  u'particularly',\n",
       "  u'notable',\n",
       "  u'for',\n",
       "  u'its',\n",
       "  u'eating',\n",
       "  u'quality',\n",
       "  u'and',\n",
       "  u'distinctive',\n",
       "  u'flavor',\n",
       "  u'and',\n",
       "  u'appearance',\n",
       "  u'the',\n",
       "  u'fruit',\n",
       "  u'is',\n",
       "  u'very',\n",
       "  u'sweet',\n",
       "  u'and',\n",
       "  u'has',\n",
       "  u'a',\n",
       "  u'pronounced',\n",
       "  u'aniseed',\n",
       "  u'flavor',\n",
       "  u'and',\n",
       "  u'takes',\n",
       "  u'on',\n",
       "  u'a',\n",
       "  u'distinctive',\n",
       "  u'red',\n",
       "  u'orange',\n",
       "  u'coloration',\n",
       "  u'as',\n",
       "  u'it',\n",
       "  u'ripens',\n",
       "  u'on',\n",
       "  u'the',\n",
       "  u'tree'],\n",
       " [u'a',\n",
       "  u'sensing',\n",
       "  u'device',\n",
       "  u'includes',\n",
       "  u'a',\n",
       "  u'circuit',\n",
       "  u'that',\n",
       "  u'compensates',\n",
       "  u'for',\n",
       "  u'time',\n",
       "  u'and',\n",
       "  u'spatial',\n",
       "  u'changes',\n",
       "  u'in',\n",
       "  u'temperature',\n",
       "  u'the',\n",
       "  u'circuit',\n",
       "  u'includes',\n",
       "  u'elements',\n",
       "  u'to',\n",
       "  u'correct',\n",
       "  u'for',\n",
       "  u'variation',\n",
       "  u'in',\n",
       "  u'permeability',\n",
       "  u'of',\n",
       "  u'a',\n",
       "  u'highly',\n",
       "  u'permeable',\n",
       "  u'core',\n",
       "  u'of',\n",
       "  u'a',\n",
       "  u'differential',\n",
       "  u'variable',\n",
       "  u'reluctance',\n",
       "  u'transducer',\n",
       "  u'as',\n",
       "  u'temperature',\n",
       "  u'changes',\n",
       "  u'the',\n",
       "  u'circuit',\n",
       "  u'also',\n",
       "  u'provides',\n",
       "  u'correction',\n",
       "  u'for',\n",
       "  u'temperature',\n",
       "  u'gradients',\n",
       "  u'across',\n",
       "  u'coils',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'transducer']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r\"(?:[A-Za-z]+[\\/\\-]?[A-Za-z]+[\\/\\-]?)+|(?:[A-Za-z]+[\\.]?){2,}|(?:[A-Za-z])+\")   #ADD\n",
    "patents_tokens = []                                                                       \n",
    "for l in ab_sent:\n",
    "    temp = []\n",
    "    for sent in l:\n",
    "        tokens = tokenizer.tokenize(sent) \n",
    "        tokens_lower = [token.lower() for token in tokens]  # lower it \n",
    "        temp.extend(tokens_lower) \n",
    "    patents_tokens.append(temp)\n",
    "\n",
    "patents_tokens[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.1-c to get tokens' information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  11360\n",
      "Total number of tokens:  281176\n",
      "Lexical diversity:  24.7514084507\n"
     ]
    }
   ],
   "source": [
    "# from itertools import chain\n",
    "words = list(chain.from_iterable(patents_tokens))\n",
    "voc = set(words)\n",
    "lexical_diversity = len(words)/len(voc)\n",
    "print \"Vocabulary size: \",len(voc)\n",
    "print \"Total number of tokens: \", len(words)\n",
    "print \"Lexical diversity: \", lexical_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Stemming and Lemmatization:\n",
    "After words tokenization, process to do the words Stemming and Lemmatization to see if any Stemming or Lemmatization procedure can help to make some word to a common base form to reduce the complexity of the word's document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-a try PorterStemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a -> a',\n",
       "  'new -> new',\n",
       "  'apple -> appl',\n",
       "  'tree -> tree',\n",
       "  'named -> name',\n",
       "  'daligris -> daligri',\n",
       "  'is -> is',\n",
       "  'disclosed -> disclos',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'new -> new',\n",
       "  'variety -> varieti',\n",
       "  'is -> is',\n",
       "  'particularly -> particularli',\n",
       "  'notable -> notabl',\n",
       "  'for -> for',\n",
       "  'its -> it',\n",
       "  'eating -> eat',\n",
       "  'quality -> qualiti',\n",
       "  'and -> and',\n",
       "  'distinctive -> distinct',\n",
       "  'flavor -> flavor',\n",
       "  'and -> and',\n",
       "  'appearance -> appear',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'is -> is',\n",
       "  'very -> veri',\n",
       "  'sweet -> sweet',\n",
       "  'and -> and',\n",
       "  'has -> ha',\n",
       "  'a -> a',\n",
       "  'pronounced -> pronounc',\n",
       "  'aniseed -> anise',\n",
       "  'flavor -> flavor',\n",
       "  'and -> and',\n",
       "  'takes -> take',\n",
       "  'on -> on',\n",
       "  'a -> a',\n",
       "  'distinctive -> distinct',\n",
       "  'red -> red',\n",
       "  'orange -> orang',\n",
       "  'coloration -> color',\n",
       "  'as -> as',\n",
       "  'it -> it',\n",
       "  'ripens -> ripen',\n",
       "  'on -> on',\n",
       "  'the -> the',\n",
       "  'tree -> tree'],\n",
       " ['a -> a',\n",
       "  'sensing -> sens',\n",
       "  'device -> devic',\n",
       "  'includes -> includ',\n",
       "  'a -> a',\n",
       "  'circuit -> circuit',\n",
       "  'that -> that',\n",
       "  'compensates -> compens',\n",
       "  'for -> for',\n",
       "  'time -> time',\n",
       "  'and -> and',\n",
       "  'spatial -> spatial',\n",
       "  'changes -> chang',\n",
       "  'in -> in',\n",
       "  'temperature -> temperatur',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'includes -> includ',\n",
       "  'elements -> element',\n",
       "  'to -> to',\n",
       "  'correct -> correct',\n",
       "  'for -> for',\n",
       "  'variation -> variat',\n",
       "  'in -> in',\n",
       "  'permeability -> permeabl',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'highly -> highli',\n",
       "  'permeable -> permeabl',\n",
       "  'core -> core',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'differential -> differenti',\n",
       "  'variable -> variabl',\n",
       "  'reluctance -> reluct',\n",
       "  'transducer -> transduc',\n",
       "  'as -> as',\n",
       "  'temperature -> temperatur',\n",
       "  'changes -> chang',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'also -> also',\n",
       "  'provides -> provid',\n",
       "  'correction -> correct',\n",
       "  'for -> for',\n",
       "  'temperature -> temperatur',\n",
       "  'gradients -> gradient',\n",
       "  'across -> across',\n",
       "  'coils -> coil',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'transducer -> transduc']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import PorterStemmer             \n",
    "stemmer = PorterStemmer()\n",
    "porter_stem = []\n",
    "for l in patents_tokens:\n",
    "    temp = ['{0} -> {1}'.format(w, stemmer.stem(w)) for w in l]\n",
    "    porter_stem.append(temp)\n",
    "porter_stem[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-b try LancasterStemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a -> a',\n",
       "  'new -> new',\n",
       "  'apple -> appl',\n",
       "  'tree -> tre',\n",
       "  'named -> nam',\n",
       "  'daligris -> daligr',\n",
       "  'is -> is',\n",
       "  'disclosed -> disclos',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'new -> new',\n",
       "  'variety -> vary',\n",
       "  'is -> is',\n",
       "  'particularly -> particul',\n",
       "  'notable -> not',\n",
       "  'for -> for',\n",
       "  'its -> it',\n",
       "  'eating -> eat',\n",
       "  'quality -> qual',\n",
       "  'and -> and',\n",
       "  'distinctive -> distinct',\n",
       "  'flavor -> flav',\n",
       "  'and -> and',\n",
       "  'appearance -> appear',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'is -> is',\n",
       "  'very -> very',\n",
       "  'sweet -> sweet',\n",
       "  'and -> and',\n",
       "  'has -> has',\n",
       "  'a -> a',\n",
       "  'pronounced -> pronount',\n",
       "  'aniseed -> anisee',\n",
       "  'flavor -> flav',\n",
       "  'and -> and',\n",
       "  'takes -> tak',\n",
       "  'on -> on',\n",
       "  'a -> a',\n",
       "  'distinctive -> distinct',\n",
       "  'red -> red',\n",
       "  'orange -> orang',\n",
       "  'coloration -> col',\n",
       "  'as -> as',\n",
       "  'it -> it',\n",
       "  'ripens -> rip',\n",
       "  'on -> on',\n",
       "  'the -> the',\n",
       "  'tree -> tre'],\n",
       " ['a -> a',\n",
       "  'sensing -> sens',\n",
       "  'device -> dev',\n",
       "  'includes -> includ',\n",
       "  'a -> a',\n",
       "  'circuit -> circuit',\n",
       "  'that -> that',\n",
       "  'compensates -> compens',\n",
       "  'for -> for',\n",
       "  'time -> tim',\n",
       "  'and -> and',\n",
       "  'spatial -> spat',\n",
       "  'changes -> chang',\n",
       "  'in -> in',\n",
       "  'temperature -> temp',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'includes -> includ',\n",
       "  'elements -> el',\n",
       "  'to -> to',\n",
       "  'correct -> correct',\n",
       "  'for -> for',\n",
       "  'variation -> vary',\n",
       "  'in -> in',\n",
       "  'permeability -> perm',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'highly -> high',\n",
       "  'permeable -> perm',\n",
       "  'core -> cor',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'differential -> diff',\n",
       "  'variable -> vary',\n",
       "  'reluctance -> reluct',\n",
       "  'transducer -> transduc',\n",
       "  'as -> as',\n",
       "  'temperature -> temp',\n",
       "  'changes -> chang',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'also -> also',\n",
       "  'provides -> provid',\n",
       "  'correction -> correct',\n",
       "  'for -> for',\n",
       "  'temperature -> temp',\n",
       "  'gradients -> grady',\n",
       "  'across -> across',\n",
       "  'coils -> coil',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'transducer -> transduc']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import LancasterStemmer                        \n",
    "stemmer = LancasterStemmer()\n",
    "lancaster_stem = []\n",
    "for l in patents_tokens:\n",
    "    temp = ['{0} -> {1}'.format(w, stemmer.stem(w)) for w in l]\n",
    "    lancaster_stem.append(temp)\n",
    "lancaster_stem[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-c try SnowballStemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a -> a',\n",
       "  'new -> new',\n",
       "  'apple -> appl',\n",
       "  'tree -> tree',\n",
       "  'named -> name',\n",
       "  'daligris -> daligri',\n",
       "  'is -> is',\n",
       "  'disclosed -> disclos',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'new -> new',\n",
       "  'variety -> varieti',\n",
       "  'is -> is',\n",
       "  'particularly -> particular',\n",
       "  'notable -> notabl',\n",
       "  'for -> for',\n",
       "  'its -> it',\n",
       "  'eating -> eat',\n",
       "  'quality -> qualiti',\n",
       "  'and -> and',\n",
       "  'distinctive -> distinct',\n",
       "  'flavor -> flavor',\n",
       "  'and -> and',\n",
       "  'appearance -> appear',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'is -> is',\n",
       "  'very -> veri',\n",
       "  'sweet -> sweet',\n",
       "  'and -> and',\n",
       "  'has -> has',\n",
       "  'a -> a',\n",
       "  'pronounced -> pronounc',\n",
       "  'aniseed -> anise',\n",
       "  'flavor -> flavor',\n",
       "  'and -> and',\n",
       "  'takes -> take',\n",
       "  'on -> on',\n",
       "  'a -> a',\n",
       "  'distinctive -> distinct',\n",
       "  'red -> red',\n",
       "  'orange -> orang',\n",
       "  'coloration -> color',\n",
       "  'as -> as',\n",
       "  'it -> it',\n",
       "  'ripens -> ripen',\n",
       "  'on -> on',\n",
       "  'the -> the',\n",
       "  'tree -> tree'],\n",
       " ['a -> a',\n",
       "  'sensing -> sens',\n",
       "  'device -> devic',\n",
       "  'includes -> includ',\n",
       "  'a -> a',\n",
       "  'circuit -> circuit',\n",
       "  'that -> that',\n",
       "  'compensates -> compens',\n",
       "  'for -> for',\n",
       "  'time -> time',\n",
       "  'and -> and',\n",
       "  'spatial -> spatial',\n",
       "  'changes -> chang',\n",
       "  'in -> in',\n",
       "  'temperature -> temperatur',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'includes -> includ',\n",
       "  'elements -> element',\n",
       "  'to -> to',\n",
       "  'correct -> correct',\n",
       "  'for -> for',\n",
       "  'variation -> variat',\n",
       "  'in -> in',\n",
       "  'permeability -> permeabl',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'highly -> high',\n",
       "  'permeable -> permeabl',\n",
       "  'core -> core',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'differential -> differenti',\n",
       "  'variable -> variabl',\n",
       "  'reluctance -> reluct',\n",
       "  'transducer -> transduc',\n",
       "  'as -> as',\n",
       "  'temperature -> temperatur',\n",
       "  'changes -> chang',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'also -> also',\n",
       "  'provides -> provid',\n",
       "  'correction -> correct',\n",
       "  'for -> for',\n",
       "  'temperature -> temperatur',\n",
       "  'gradients -> gradient',\n",
       "  'across -> across',\n",
       "  'coils -> coil',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'transducer -> transduc']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "snowball_stem = []\n",
    "for l in patents_tokens:\n",
    "    temp = ['{0} -> {1}'.format(w, stemmer.stem(w)) for w in l]\n",
    "    snowball_stem.append(temp)\n",
    "snowball_stem[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-d try WordNetLemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a -> a',\n",
       "  'new -> new',\n",
       "  'apple -> apple',\n",
       "  'tree -> tree',\n",
       "  'named -> name',\n",
       "  'daligris -> daligris',\n",
       "  'is -> be',\n",
       "  'disclosed -> disclose',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'new -> new',\n",
       "  'variety -> variety',\n",
       "  'is -> be',\n",
       "  'particularly -> particularly',\n",
       "  'notable -> notable',\n",
       "  'for -> for',\n",
       "  'its -> it',\n",
       "  'eating -> eating',\n",
       "  'quality -> quality',\n",
       "  'and -> and',\n",
       "  'distinctive -> distinctive',\n",
       "  'flavor -> flavor',\n",
       "  'and -> and',\n",
       "  'appearance -> appearance',\n",
       "  'the -> the',\n",
       "  'fruit -> fruit',\n",
       "  'is -> be',\n",
       "  'very -> very',\n",
       "  'sweet -> sweet',\n",
       "  'and -> and',\n",
       "  'has -> have',\n",
       "  'a -> a',\n",
       "  'pronounced -> pronounce',\n",
       "  'aniseed -> aniseed',\n",
       "  'flavor -> flavor',\n",
       "  'and -> and',\n",
       "  'takes -> take',\n",
       "  'on -> on',\n",
       "  'a -> a',\n",
       "  'distinctive -> distinctive',\n",
       "  'red -> red',\n",
       "  'orange -> orange',\n",
       "  'coloration -> coloration',\n",
       "  'as -> a',\n",
       "  'it -> it',\n",
       "  'ripens -> ripen',\n",
       "  'on -> on',\n",
       "  'the -> the',\n",
       "  'tree -> tree'],\n",
       " ['a -> a',\n",
       "  'sensing -> sensing',\n",
       "  'device -> device',\n",
       "  'includes -> include',\n",
       "  'a -> a',\n",
       "  'circuit -> circuit',\n",
       "  'that -> that',\n",
       "  'compensates -> compensate',\n",
       "  'for -> for',\n",
       "  'time -> time',\n",
       "  'and -> and',\n",
       "  'spatial -> spatial',\n",
       "  'changes -> change',\n",
       "  'in -> in',\n",
       "  'temperature -> temperature',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'includes -> include',\n",
       "  'elements -> element',\n",
       "  'to -> to',\n",
       "  'correct -> correct',\n",
       "  'for -> for',\n",
       "  'variation -> variation',\n",
       "  'in -> in',\n",
       "  'permeability -> permeability',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'highly -> highly',\n",
       "  'permeable -> permeable',\n",
       "  'core -> core',\n",
       "  'of -> of',\n",
       "  'a -> a',\n",
       "  'differential -> differential',\n",
       "  'variable -> variable',\n",
       "  'reluctance -> reluctance',\n",
       "  'transducer -> transducer',\n",
       "  'as -> a',\n",
       "  'temperature -> temperature',\n",
       "  'changes -> change',\n",
       "  'the -> the',\n",
       "  'circuit -> circuit',\n",
       "  'also -> also',\n",
       "  'provides -> provide',\n",
       "  'correction -> correction',\n",
       "  'for -> for',\n",
       "  'temperature -> temperature',\n",
       "  'gradients -> gradient',\n",
       "  'across -> across',\n",
       "  'coils -> coil',\n",
       "  'of -> of',\n",
       "  'the -> the',\n",
       "  'transducer -> transducer']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get pos tags to all tokens\n",
    "# import nltk\n",
    "patents_tokens_pos = []\n",
    "for l in patents_tokens:\n",
    "    temp = nltk.pos_tag(l)\n",
    "    patents_tokens_pos.append (temp)\n",
    "\n",
    "# Function for change the pos tags for WordNetLemmatizer can regonize in Lemmatization procedure   \n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "#Do Lemmatization\n",
    "# from nltk.stem import WordNetLemmatizer       \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordNet_lemma = []\n",
    "for l in patents_tokens_pos:\n",
    "    temp = ['{0} -> {1}'.format(w[0], lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1]))) for w in l]\n",
    "    wordNet_lemma.append(temp)\n",
    "wordNet_lemma[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-e \n",
    "###### to get tokens information after getting the result of all Stemming and Lemmatization, I found that WordNetLemmatizer can do a better job in words formalization for example: \n",
    "* named -> name\n",
    "* pronounced -> pronounce\n",
    "* means -> mean\n",
    "* receives -> receive\n",
    "\n",
    "###### while three of Stemmers all return some wired transformation of the words, for example:\n",
    "* changes -> chang\n",
    "* permeability -> permeabl\n",
    "* variable -> variabl,\n",
    "* reluctance -> reluct,\n",
    "* transducer -> transduc, and so on. \n",
    "\n",
    "###### which are not useful to make words to a common base form to reduce the complexity of the word's document. \n",
    "###### So, I choose to do the Lemmatization by using WordNetLemmatizer and not using Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'a',\n",
       "  u'new',\n",
       "  u'apple',\n",
       "  u'tree',\n",
       "  u'name',\n",
       "  u'daligris',\n",
       "  u'be',\n",
       "  u'disclose',\n",
       "  u'the',\n",
       "  u'fruit',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'new',\n",
       "  u'variety',\n",
       "  u'be',\n",
       "  u'particularly',\n",
       "  u'notable',\n",
       "  u'for',\n",
       "  u'it',\n",
       "  u'eating',\n",
       "  u'quality',\n",
       "  u'and',\n",
       "  u'distinctive',\n",
       "  u'flavor',\n",
       "  u'and',\n",
       "  u'appearance',\n",
       "  u'the',\n",
       "  u'fruit',\n",
       "  u'be',\n",
       "  u'very',\n",
       "  u'sweet',\n",
       "  u'and',\n",
       "  u'have',\n",
       "  u'a',\n",
       "  u'pronounce',\n",
       "  u'aniseed',\n",
       "  u'flavor',\n",
       "  u'and',\n",
       "  u'take',\n",
       "  u'on',\n",
       "  u'a',\n",
       "  u'distinctive',\n",
       "  u'red',\n",
       "  u'orange',\n",
       "  u'coloration',\n",
       "  u'a',\n",
       "  u'it',\n",
       "  u'ripen',\n",
       "  u'on',\n",
       "  u'the',\n",
       "  u'tree'],\n",
       " [u'a',\n",
       "  u'sensing',\n",
       "  u'device',\n",
       "  u'include',\n",
       "  u'a',\n",
       "  u'circuit',\n",
       "  u'that',\n",
       "  u'compensate',\n",
       "  u'for',\n",
       "  u'time',\n",
       "  u'and',\n",
       "  u'spatial',\n",
       "  u'change',\n",
       "  u'in',\n",
       "  u'temperature',\n",
       "  u'the',\n",
       "  u'circuit',\n",
       "  u'include',\n",
       "  u'element',\n",
       "  u'to',\n",
       "  u'correct',\n",
       "  u'for',\n",
       "  u'variation',\n",
       "  u'in',\n",
       "  u'permeability',\n",
       "  u'of',\n",
       "  u'a',\n",
       "  u'highly',\n",
       "  u'permeable',\n",
       "  u'core',\n",
       "  u'of',\n",
       "  u'a',\n",
       "  u'differential',\n",
       "  u'variable',\n",
       "  u'reluctance',\n",
       "  u'transducer',\n",
       "  u'a',\n",
       "  u'temperature',\n",
       "  u'change',\n",
       "  u'the',\n",
       "  u'circuit',\n",
       "  u'also',\n",
       "  u'provide',\n",
       "  u'correction',\n",
       "  u'for',\n",
       "  u'temperature',\n",
       "  u'gradient',\n",
       "  u'across',\n",
       "  u'coil',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'transducer']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents_tokens_lemma = []\n",
    "\n",
    "for l in patents_tokens_pos:\n",
    "    temp = [lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) for w in l]\n",
    "    patents_tokens_lemma.append(temp)\n",
    "    \n",
    "patents_tokens_lemma[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-f to get tokens' information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9474\n",
      "Total number of tokens:  281176\n",
      "Lexical diversity:  29.6786995989\n"
     ]
    }
   ],
   "source": [
    "# from itertools import chain\n",
    "words_1 = list(chain.from_iterable(patents_tokens_lemma))\n",
    "voc_1 = set(words_1)\n",
    "lexical_diversity_1 = len(words_1)/len(voc_1)\n",
    "print \"Vocabulary size: \",len(voc_1)\n",
    "print \"Total number of tokens: \", len(words_1)\n",
    "print \"Lexical diversity: \", lexical_diversity_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-g Have a quick look for the words frequency distribution: We found out the most common words do belong to the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 25766),\n",
       " (u'a', 19750),\n",
       " (u'of', 10129),\n",
       " (u'and', 8793),\n",
       " (u'be', 8096),\n",
       " (u'to', 7481),\n",
       " (u'in', 4134),\n",
       " (u'an', 4046),\n",
       " (u'for', 2986),\n",
       " (u'first', 2658),\n",
       " (u'include', 2325),\n",
       " (u'with', 2143),\n",
       " (u'second', 2110),\n",
       " (u'have', 1940),\n",
       " (u'that', 1845),\n",
       " (u'at', 1748),\n",
       " (u'on', 1731),\n",
       " (u'from', 1674),\n",
       " (u'by', 1581),\n",
       " (u'one', 1539)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = FreqDist(words_1)            \n",
    "fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-h To exclude the stopwords and see the words frequency distribution again:\n",
    "Found that word \"include\" which appears 2325 times is the most common word now. However, this word as a verb, also not\n",
    "providing any meaningful information for the text analysis of patent's citation network. So I will remove it by treating it as a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'include', 2325),\n",
       " (u'device', 1461),\n",
       " (u'portion', 1346),\n",
       " (u'provide', 1273),\n",
       " (u'signal', 1119),\n",
       " (u'system', 1088),\n",
       " (u'form', 1025),\n",
       " (u'surface', 997),\n",
       " (u'data', 972),\n",
       " (u'method', 959),\n",
       " (u'control', 929),\n",
       " (u'member', 889),\n",
       " (u'unit', 886),\n",
       " (u'layer', 867),\n",
       " (u'position', 811),\n",
       " (u'plurality', 805),\n",
       " (u'end', 740),\n",
       " (u'circuit', 704),\n",
       " (u'connect', 695),\n",
       " (u'base', 676)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwordsSet = set(stopwords)\n",
    "\n",
    "words_WithOutStopwords = [item for item in words_1 if item not in stopwordsSet ] \n",
    "fd_1 = FreqDist(words_WithOutStopwords)            \n",
    "fd_1.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2-i Updating word \"include\" into the stopwords list it the stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsSet.add(\"include\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Producing meaningful bigram collocation\n",
    "\n",
    "I have obtained abstracts' tokens from tokenization process and make tokens to the common form base by Lemmatization. \n",
    "In step 3.3, I'm going to use some technique to found some meaningful bigram from this all unigram token list.\n",
    "###### 3.3-a Look for suitable bigram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'first', u'second'),\n",
       " (u'present', u'invention'),\n",
       " (u'layer', u'layer'),\n",
       " (u'signal', u'signal'),\n",
       " (u'portion', u'portion'),\n",
       " (u'memory', u'cell'),\n",
       " (u'data', u'data'),\n",
       " (u'member', u'member'),\n",
       " (u'image', u'image'),\n",
       " (u'region', u'region')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words_1, window_size = 10)  \n",
    "finder.apply_freq_filter(4)   #filter out the word which is not appear more than 4 times \n",
    "finder.apply_word_filter(lambda w: len(w) < 4)   #filter out the word which is not longer then 4 characters\n",
    "bigram = finder.nbest(bigram_measures.likelihood_ratio, 300) \n",
    "bigram[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.3-b \n",
    "###### I found that it returns some good and suitable bigram which can be used to update the tokens list such as:\n",
    "* memory cell\n",
    "* semiconductor substrate\n",
    "* laser beam\n",
    "* circuit board\n",
    "* wind turbine\n",
    "\n",
    "###### However, it also returns bigrams that which are not meaningful for some bigram contain stopword or bigram are combined with two some words such as:\n",
    "* first second\n",
    "* such that\n",
    "* great than\n",
    "* data data\n",
    "* valve valve\n",
    "* heat heat\n",
    "\n",
    "###### Removed all unwanted bigrams and retain the meaningful bigrams only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'present', u'invention'),\n",
       " (u'memory', u'cell'),\n",
       " (u'light', u'emit'),\n",
       " (u'light', u'source'),\n",
       " (u'semiconductor', u'substrate'),\n",
       " (u'invention', u'relate'),\n",
       " (u'power', u'supply'),\n",
       " (u'liquid', u'crystal'),\n",
       " (u'integrate', u'circuit'),\n",
       " (u'electrically', u'connect')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove bigrams which have stopwords as a not meaningful bigram\n",
    "bigram_2 = [item for item in bigram if item[0] not in stopwordsSet and item[1] not in stopwordsSet] \n",
    "\n",
    "# remove bigrams which have two same words paired as a not a meaningful bigram\n",
    "bigram_fn = [item for item in bigram_2 if item[0] != item[1]] \n",
    "\n",
    "print len(bigram_fn)\n",
    "bigram_fn[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.3-c Produce a vocabulary list with the new adding 123 set of bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'four',\n",
       " u'prefix',\n",
       " u'circuitry',\n",
       " u'hanging',\n",
       " u'backend-tier',\n",
       " u'semi-hermetic',\n",
       " u'localized',\n",
       " u'hingedly',\n",
       " u'electricity',\n",
       " u'originality']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_with_bigram = list(voc_1)\n",
    "voc_with_bigram += [ bigram for bigram in bigram_fn]  \n",
    "voc_with_bigram[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Removing Stop words\n",
    "\n",
    "###### Tokens belong to the stopwords will be removed from the patents_tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_tokens_lemma_mvStopwords = []\n",
    "for l in patents_tokens_lemma:\n",
    "    temp = [token for token in l if token not in stopwordsSet]\n",
    "    patents_tokens_lemma_mvStopwords.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 Tokenization (tokens updated to bigrams)\n",
    "###### Update words to bigrams for the selected words combination from the bigrams list to the patents_tokens list by using MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'apple',\n",
       "  u'tree',\n",
       "  u'daligris',\n",
       "  u'disclose',\n",
       "  u'fruit',\n",
       "  u'variety',\n",
       "  u'notable',\n",
       "  u'eating',\n",
       "  u'quality',\n",
       "  u'distinctive',\n",
       "  u'flavor',\n",
       "  u'appearance',\n",
       "  u'fruit',\n",
       "  u'sweet',\n",
       "  u'pronounce',\n",
       "  u'aniseed',\n",
       "  u'flavor',\n",
       "  u'distinctive',\n",
       "  u'red',\n",
       "  u'orange',\n",
       "  u'coloration',\n",
       "  u'ripen',\n",
       "  u'tree'],\n",
       " [u'sensing',\n",
       "  u'device',\n",
       "  u'circuit',\n",
       "  u'compensate',\n",
       "  u'time',\n",
       "  u'spatial',\n",
       "  u'change',\n",
       "  u'temperature',\n",
       "  u'circuit',\n",
       "  u'element',\n",
       "  u'correct',\n",
       "  u'variation',\n",
       "  u'permeability',\n",
       "  u'highly',\n",
       "  u'permeable',\n",
       "  u'core',\n",
       "  u'differential',\n",
       "  u'variable',\n",
       "  u'reluctance',\n",
       "  u'transducer',\n",
       "  u'temperature',\n",
       "  u'change',\n",
       "  u'circuit',\n",
       "  u'provide',\n",
       "  u'correction',\n",
       "  u'temperature',\n",
       "  u'gradient',\n",
       "  u'coil',\n",
       "  u'transducer']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.tokenize import MWETokenizer\n",
    "mwe_tokenizer = MWETokenizer(voc_with_bigram)     \n",
    "patents_tokens_lemma_mvStopwords_2 =[mwe_tokenizer.tokenize(tokens) for tokens in patents_tokens_lemma_mvStopwords] \n",
    "patents_tokens_lemma_mvStopwords_2[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.6 Removing the Most and Less Frequent Words\n",
    " * top-20 most frequent words based on word’s document frequency\n",
    " * words only appearing in one abstract as required\n",
    "\n",
    "###### 3.6-a To get the list for top 20 most frequent words based on word’s document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'portion',\n",
       " u'provide',\n",
       " u'device',\n",
       " u'system',\n",
       " u'form',\n",
       " u'surface',\n",
       " u'data',\n",
       " u'signal',\n",
       " u'member',\n",
       " u'position']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from itertools import chain\n",
    "words_2 = list(chain.from_iterable(patents_tokens_lemma_mvStopwords_2))\n",
    "fd_2 = FreqDist(words_2)            \n",
    "top_20_frequent_words= [ w[0] for w in fd_2.most_common(20) ]\n",
    "top_20_frequent_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.6-b To get the list for words only appearing in one abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'transit_tier',\n",
       " u'backend-tier',\n",
       " u'semi-hermetic',\n",
       " u'localized',\n",
       " u'originality',\n",
       " u'image-side',\n",
       " u'crossbar',\n",
       " u'sputter',\n",
       " u'herbicide',\n",
       " u'collate']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patents_tokens_lemma_mvStopwords_2_set = list(chain.from_iterable([set(value) for value in patents_tokens_lemma_mvStopwords_2]))\n",
    "fd_3 = FreqDist(patents_tokens_lemma_mvStopwords_2_set)\n",
    "word_only_appear_in_one = [ w for w in fd_3.hapaxes()]\n",
    "word_only_appear_in_one[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.6-c To remove words in patents tokens list for the words in to 2 word removing list \"top_20_frequent_words\"  and \"word_only_appear_in_one\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'tree',\n",
       "  u'disclose',\n",
       "  u'fruit',\n",
       "  u'variety',\n",
       "  u'eating',\n",
       "  u'quality',\n",
       "  u'flavor',\n",
       "  u'appearance',\n",
       "  u'fruit',\n",
       "  u'flavor',\n",
       "  u'red',\n",
       "  u'coloration',\n",
       "  u'tree'],\n",
       " [u'sensing',\n",
       "  u'circuit',\n",
       "  u'compensate',\n",
       "  u'time',\n",
       "  u'spatial',\n",
       "  u'change',\n",
       "  u'temperature',\n",
       "  u'circuit',\n",
       "  u'element',\n",
       "  u'correct',\n",
       "  u'variation',\n",
       "  u'highly',\n",
       "  u'permeable',\n",
       "  u'core',\n",
       "  u'differential',\n",
       "  u'variable',\n",
       "  u'transducer',\n",
       "  u'temperature',\n",
       "  u'change',\n",
       "  u'circuit',\n",
       "  u'correction',\n",
       "  u'temperature',\n",
       "  u'gradient',\n",
       "  u'coil',\n",
       "  u'transducer']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_frequent_words = set(top_20_frequent_words)\n",
    "word_only_appear_in_one = set(word_only_appear_in_one)\n",
    "\n",
    "patents_tokens_fn = []\n",
    "for l in patents_tokens_lemma_mvStopwords_2 :\n",
    "    temp = [token for token in l if token not in top_20_frequent_words and token not in word_only_appear_in_one]\n",
    "    patents_tokens_fn.append (temp)  \n",
    "    \n",
    "patents_tokens_fn[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.6-d to get tokens' information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  4436\n",
      "Total number of tokens:  116868\n",
      "Lexical diversity:  26.3453561767\n"
     ]
    }
   ],
   "source": [
    "# from itertools import chain\n",
    "words_fn = list(chain.from_iterable(patents_tokens_fn))\n",
    "voc_fn = set(words_fn)\n",
    "lexical_diversity_fn = len(words_fn)/len(voc_fn)\n",
    "print \"Vocabulary size: \",len(voc_fn)\n",
    "print \"Total number of tokens: \", len(words_fn)\n",
    "print \"Lexical diversity: \", lexical_diversity_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Data formatting for the collected data and output to file.\n",
    "\n",
    "##### 4.1 Now a final tokens have been set up with the words we want to retain only. To combine this tokens to its patent id number together in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_ID_tokens = { patents_id[i]: patents_tokens_fn[i] for i in range (0,len(patents_tokens_fn))}\n",
    "#len(patent_ID_tokens)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  4.2 Generating the sparse count vectors and output to file “count_vectors.txt” where each row corresponds to a patent’s abstract, starting with patent_id, followed by “word_index:count” pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open (\"count_vectors.txt\", \"w+\")\n",
    "\n",
    "voc_fn = list(voc_fn)\n",
    "voc_fn.sort()\n",
    "for key in patent_ID_tokens.keys():\n",
    "    output_file.write(key+\":\")\n",
    "    d_idx = [voc_fn.index(w) for w in patent_ID_tokens[key]]\n",
    "    for k, v in FreqDist(d_idx).iteritems():\n",
    "        output_file.write(\"{}:{} \".format(k,v))\n",
    "    output_file.write('\\n')\n",
    "    \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  4.3 Generating vocabulary list to file “vocab.txt” with all vocabulary where each row corresponds to a vocabulary, starting with word_index followed by word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = open (\"vocab.txt\", \"w+\")\n",
    "\n",
    "for i in range (0, len(voc_fn)):\n",
    "    output_file.write(str(i) + \" : \" + voc_fn[i] + \"\\n\")        \n",
    "    \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion after text preprocessing task\n",
    "\n",
    "After finished the text preprocessing task, we have not only extract out data we have, but also remove some not useful words by adopted some text preprocessing strategy and techniques, the final token list is in a better volume and with a better standardize case and word form. The final token that stored in an output file can then be used by the Data Analyst for preforming analysis to get some information from this pre-processed data.  \n",
    "\n",
    "Below shown the different stage of words and vocabularies volume. Total number of tokens in the document reduced from 281176 to 116868 while Vocabulary size reduced from 11360 to 4436."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens forom abstracts: \n",
      "Vocabulary size:  11360\n",
      "Total number of tokens:  281176\n",
      "Lexical diversity:  24.7514084507 \n",
      "\n",
      "Tokens after Lemmatization: \n",
      "Vocabulary size:  9474\n",
      "Total number of tokens:  281176\n",
      "Lexical diversity:  29.6786995989 \n",
      "\n",
      "Final tokens after text pre-processing: \n",
      "Vocabulary size:  4436\n",
      "Total number of tokens:  116868\n",
      "Lexical diversity:  26.3453561767\n"
     ]
    }
   ],
   "source": [
    "print \"Original tokens forom abstracts: \"\n",
    "print \"Vocabulary size: \",len(voc)\n",
    "print \"Total number of tokens: \", len(words)\n",
    "print \"Lexical diversity: \", lexical_diversity , \"\\n\"\n",
    "\n",
    "print \"Tokens after Lemmatization: \"\n",
    "print \"Vocabulary size: \",len(voc_1)\n",
    "print \"Total number of tokens: \", len(words_1)\n",
    "print \"Lexical diversity: \", lexical_diversity_1, \"\\n\"\n",
    "\n",
    "print \"Final tokens after text pre-processing: \"\n",
    "print \"Vocabulary size: \",len(voc_fn)\n",
    "print \"Total number of tokens: \", len(words_fn)\n",
    "print \"Lexical diversity: \", lexical_diversity_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
